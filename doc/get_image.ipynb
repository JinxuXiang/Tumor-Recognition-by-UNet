{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 5481,
     "status": "ok",
     "timestamp": 1604950314237,
     "user": {
      "displayName": "项津旭",
      "photoUrl": "",
      "userId": "05816301843889399017"
     },
     "user_tz": 300
    },
    "id": "rrFXyyroK2dF"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from openslide import open_slide, __library_version__ as openslide_version\n",
    "import os\n",
    "from PIL import Image\n",
    "from skimage.color import rgb2gray\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = 'C:/course/4995 Applied Deeping Learning/homework/Project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 5476,
     "status": "ok",
     "timestamp": 1604950314237,
     "user": {
      "displayName": "项津旭",
      "photoUrl": "",
      "userId": "05816301843889399017"
     },
     "user_tz": 300
    },
    "id": "wlfy42DpMbk0"
   },
   "outputs": [],
   "source": [
    "def read_slide(slide, x, y, level, width, height, as_float=False, rotation = 0):\n",
    "    im = slide.read_region((x,y), level, (width, height)) \n",
    "    # x,y to (61440,53760) original display area\n",
    "    # width,height image size\n",
    "    im = im.convert('RGB') # drop the alpha channel\n",
    "    im = im.rotate(rotation)\n",
    "    if as_float:\n",
    "        im = np.asarray(im, dtype=np.float32)\n",
    "    else:\n",
    "        im = np.asarray(im)\n",
    "    assert im.shape == (height, width, 3)\n",
    "    return im\n",
    "\n",
    "def find_tissue_pixels(image, intensity_low=0.1, intensity_high=0.8):\n",
    "    im_gray = rgb2gray(image)\n",
    "    assert im_gray.shape == (image.shape[0], image.shape[1])\n",
    "    pos_min = im_gray >= intensity_low\n",
    "    pos_max = im_gray <= intensity_high\n",
    "    pos_rst = pos_min & pos_max\n",
    "    indices = np.where(pos_rst == True)\n",
    "    return list(zip(indices[0], indices[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 1140,
     "status": "ok",
     "timestamp": 1604955921020,
     "user": {
      "displayName": "项津旭",
      "photoUrl": "",
      "userId": "05816301843889399017"
     },
     "user_tz": 300
    },
    "id": "j_jzUNHmLAGQ"
   },
   "outputs": [],
   "source": [
    "def crop_image_data(slide_path, tumor_mask_path, level_img = 2, width_crop = 256, height_crop = 256, width_move = 100, height_move = 100, category = 'train', tumer_num = '000'):\n",
    "  global image_index\n",
    "  image_index_start = image_index\n",
    "\n",
    "  slide = open_slide(slide_path)\n",
    "  print (\"Read WSI from %s with width: %d, height: %d\" % (slide_path, slide.level_dimensions[0][0], slide.level_dimensions[0][1]))\n",
    "\n",
    "  tumor_mask = open_slide(tumor_mask_path)\n",
    "  print (\"Read tumor mask from %s\" % (tumor_mask_path))\n",
    "\n",
    "  print(\"Slide includes %d levels\" % len(slide.level_dimensions), \"Mask includes %d levels\" % len(tumor_mask.level_dimensions))\n",
    "  for i in range(min(len(slide.level_dimensions),len(tumor_mask.level_dimensions))):\n",
    "      print(\"Level %d, dimensions: %s downsample factor %d\" % (i, slide.level_dimensions[i], slide.level_downsamples[i]))\n",
    "      assert tumor_mask.level_dimensions[i][0] == slide.level_dimensions[i][0]\n",
    "      assert tumor_mask.level_dimensions[i][1] == slide.level_dimensions[i][1]\n",
    "\n",
    "  # Verify downsampling works as expected\n",
    "  width, height = slide.level_dimensions[level_img]\n",
    "  assert width * slide.level_downsamples[level_img] == slide.level_dimensions[0][0]\n",
    "  assert height * slide.level_downsamples[level_img] == slide.level_dimensions[0][1]\n",
    "\n",
    "  # slide.shape = (height, width, 3), mask.shape = (height, width)\n",
    "  slide_image = read_slide(slide, x=0, y=0, level=level_img, width=slide.level_dimensions[level_img][0], height=slide.level_dimensions[level_img][1])\n",
    "  mask_image = read_slide(tumor_mask, x=0, y=0, level=level_img, width=slide.level_dimensions[level_img][0], height=slide.level_dimensions[level_img][1])\n",
    "  mask_image = mask_image[:,:,0]\n",
    "\n",
    "  height_num = (height-int(height_crop*1.5))//height_move\n",
    "  height_range = np.arange(height_num-1) * height_move + height_move\n",
    "  width_num = (width-int(width_crop*1.5))//width_move\n",
    "  width_range = np.arange(width_num-1) * width_move + width_move\n",
    "\n",
    "  crop_number = len(height_range)*len(width_range)\n",
    "  print('Crop to %d images.' % crop_number)\n",
    "\n",
    "  # weights[0]: mask==0, weights[1]: mask == 1\n",
    "  weights = np.asarray([0,0],dtype=np.int64)\n",
    "\n",
    "  image_area = width_crop*height_crop\n",
    "  image_start_pos = []\n",
    "  count = 0\n",
    "  for h in height_range:\n",
    "    for w in width_range:\n",
    "      if count % max((crop_number//20),1) == 0:\n",
    "        print('.',end='')\n",
    "      count += 1\n",
    "\n",
    "      hrand = int(np.random.uniform(low = -height_crop/4, high= height_crop/4))\n",
    "      wrand = int(np.random.uniform(low = -width_crop/4, high= width_crop/4))\n",
    "      now_w = int((w+wrand)*slide.level_downsamples[level_img])\n",
    "      now_h = int((h+hrand)*slide.level_downsamples[level_img])\n",
    "      image_crop = read_slide(slide, x=now_w, y=now_h, level=level_img, width=width_crop, height=height_crop)\n",
    "      tissue_pixels = find_tissue_pixels(image_crop)\n",
    "      percent_tissue = len(tissue_pixels) / float(image_area)\n",
    "      if percent_tissue > 0.1:\n",
    "        mask_crop = read_slide(tumor_mask, x=now_w, y=now_h, level=level_img, width=width_crop, height=height_crop)\n",
    "        mask_crop = mask_crop[:,:,0]\n",
    "        if np.mean(mask_crop) > 0:\n",
    "          image_start_pos += [(int(tumer_num),h+hrand,w+wrand)]\n",
    "          masked = np.sum(mask_crop>=0.5)\n",
    "          image_crop = Image.fromarray(image_crop, 'RGB')\n",
    "          mask_crop = Image.fromarray(mask_crop*255, 'L')\n",
    "          image_crop.save(my_path+'/Crop_Data/level'+str(level_img)+'/'+category+'/image/'+str(image_index).zfill(5)+'.jpg', 'JPEG')\n",
    "          mask_crop.save(my_path+'/Crop_Data/level'+str(level_img)+'/'+category+'/mask/'+str(image_index).zfill(5)+'.jpg', 'JPEG')\n",
    "          weights[0] += image_area - masked\n",
    "          weights[1] += masked\n",
    "\n",
    "          ### high level image +2\n",
    "          now_w_up2 = int(now_w)\n",
    "          now_h_up2 = int(now_h)\n",
    "          image_crop_up2 = read_slide(slide, x=now_w_up2, y=now_h_up2, level=level_img+2, width=width_crop//4, height=height_crop//4)\n",
    "          mask_crop_up2 = read_slide(tumor_mask, x=now_w_up2, y=now_h_up2, level=level_img+2, width=width_crop//4, height=height_crop//4)\n",
    "          mask_crop_up2 = mask_crop_up2[:,:,0]\n",
    "          image_crop_up2 = Image.fromarray(image_crop_up2, 'RGB')\n",
    "          mask_crop_up2 = Image.fromarray(mask_crop_up2*255, 'L')\n",
    "          image_crop_up2.save(my_path+'/Crop_Data/level'+str(level_img+2)+'/'+category+'/image/'+str(image_index).zfill(5)+'.jpg', 'JPEG')\n",
    "          mask_crop_up2.save(my_path+'/Crop_Data/level'+str(level_img+2)+'/'+category+'/mask/'+str(image_index).zfill(5)+'.jpg', 'JPEG')\n",
    "\n",
    "          ### high level image +4\n",
    "          now_w_up4 = int(now_w)\n",
    "          now_h_up4 = int(now_h)\n",
    "          image_crop_up4 = read_slide(slide, x=now_w_up4, y=now_h_up4, level=level_img+4, width=width_crop//16, height=height_crop//16)\n",
    "          mask_crop_up4 = read_slide(tumor_mask, x=now_w_up4, y=now_h_up4, level=level_img+4, width=width_crop//16, height=height_crop//16)\n",
    "          mask_crop_up4 = mask_crop_up4[:,:,0]\n",
    "          image_crop_up4 = Image.fromarray(image_crop_up4, 'RGB')\n",
    "          mask_crop_up4 = Image.fromarray(mask_crop_up4*255, 'L')\n",
    "          image_crop_up4.save(my_path+'/Crop_Data/level'+str(level_img+4)+'/'+category+'/image/'+str(image_index).zfill(5)+'.jpg', 'JPEG')\n",
    "          mask_crop_up4.save(my_path+'/Crop_Data/level'+str(level_img+4)+'/'+category+'/mask/'+str(image_index).zfill(5)+'.jpg', 'JPEG')\n",
    "\n",
    "          image_index += 1\n",
    "        else:\n",
    "          u = np.random.uniform()\n",
    "          if u < 0.5:\n",
    "            image_start_pos += [(int(tumer_num),h+hrand,w+wrand)]\n",
    "            image_crop = Image.fromarray(image_crop, 'RGB')\n",
    "            mask_crop = Image.fromarray(mask_crop*255, 'L')\n",
    "            image_crop.save(my_path+'/Crop_Data/level'+str(level_img)+'/'+category+'/image/'+str(image_index).zfill(5)+'.jpg', 'JPEG')\n",
    "            mask_crop.save(my_path+'/Crop_Data/level'+str(level_img)+'/'+category+'/mask/'+str(image_index).zfill(5)+'.jpg', 'JPEG')\n",
    "            weights[0] += image_area\n",
    "\n",
    "            ### high level image +2\n",
    "            now_w_up2 = int(now_w)\n",
    "            now_h_up2 = int(now_h)\n",
    "            image_crop_up2 = read_slide(slide, x=now_w_up2, y=now_h_up2, level=level_img+2, width=width_crop//4, height=height_crop//4)\n",
    "            mask_crop_up2 = read_slide(tumor_mask, x=now_w_up2, y=now_h_up2, level=level_img+2, width=width_crop//4, height=height_crop//4)\n",
    "            mask_crop_up2 = mask_crop_up2[:,:,0]\n",
    "            image_crop_up2 = Image.fromarray(image_crop_up2, 'RGB')\n",
    "            mask_crop_up2 = Image.fromarray(mask_crop_up2*255, 'L')\n",
    "            image_crop_up2.save(my_path+'/Crop_Data/level'+str(level_img+2)+'/'+category+'/image/'+str(image_index).zfill(5)+'.jpg', 'JPEG')\n",
    "            mask_crop_up2.save(my_path+'/Crop_Data/level'+str(level_img+2)+'/'+category+'/mask/'+str(image_index).zfill(5)+'.jpg', 'JPEG')\n",
    "            \n",
    "            ### high level image +4\n",
    "            now_w_up4 = int(now_w)\n",
    "            now_h_up4 = int(now_h)\n",
    "            image_crop_up4 = read_slide(slide, x=now_w_up4, y=now_h_up4, level=level_img+4, width=width_crop//16, height=height_crop//16)\n",
    "            mask_crop_up4 = read_slide(tumor_mask, x=now_w_up4, y=now_h_up4, level=level_img+4, width=width_crop//16, height=height_crop//16)\n",
    "            mask_crop_up4 = mask_crop_up4[:,:,0]\n",
    "            image_crop_up4 = Image.fromarray(image_crop_up4, 'RGB')\n",
    "            mask_crop_up4 = Image.fromarray(mask_crop_up4*255, 'L')\n",
    "            image_crop_up4.save(my_path+'/Crop_Data/level'+str(level_img+4)+'/'+category+'/image/'+str(image_index).zfill(5)+'.jpg', 'JPEG')\n",
    "            mask_crop_up4.save(my_path+'/Crop_Data/level'+str(level_img+4)+'/'+category+'/mask/'+str(image_index).zfill(5)+'.jpg', 'JPEG')\n",
    "\n",
    "            image_index += 1\n",
    "  \n",
    "  print('')\n",
    "  print('Weight proportion： %d:%d' %(weights[0], weights[1]))\n",
    "  print('Generate %d valid images' %int(image_index - image_index_start))\n",
    "\n",
    "  return image_start_pos, weights\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 1146,
     "status": "ok",
     "timestamp": 1604955170062,
     "user": {
      "displayName": "项津旭",
      "photoUrl": "",
      "userId": "05816301843889399017"
     },
     "user_tz": 300
    },
    "id": "8Gk97D8mYGNo"
   },
   "outputs": [],
   "source": [
    "def crop_train_test_data(tumor_num=['000'], level_img = 5, width_crop = 256, height_crop = 256, width_move = 100, height_move = 100, category = 'train'):\n",
    "  global image_index\n",
    "\n",
    "  image_index = 0\n",
    "  image_start_pos = []\n",
    "  weights = np.array([0,0])\n",
    "  for t in tumor_num:\n",
    "    slide_path = my_path+'/Project Data/tumor_' + t + '.tif'\n",
    "    tumor_mask_path = my_path+'/Project Data/tumor_' + t + '_mask.tif'\n",
    "    image_start_pos_t, new_weights = crop_image_data(slide_path, tumor_mask_path, level_img=level_img, width_crop=width_crop, height_crop=height_crop, width_move=width_move, height_move=height_move, category=category, tumer_num=t)\n",
    "    image_start_pos += image_start_pos_t\n",
    "    weights += np.array(new_weights)\n",
    "    print('Finish tumor image:', t)\n",
    "    print('')\n",
    "  \n",
    "  print('Weight proportion on %s data： %d:%d' %(category, weights[0], weights[1]))\n",
    "  print('A total of %d %s data is generated.' % (image_index, category))\n",
    "  print('')\n",
    "  np.save(my_path+'/Crop_Data/level'+str(level)+'/'+category+'/image_start_pos.npy',image_start_pos)\n",
    "  np.save(my_path+'/Crop_Data/level'+str(level)+'/'+category+'/weights.npy',weights)\n",
    "  return image_start_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 1038,
     "status": "ok",
     "timestamp": 1604955219608,
     "user": {
      "displayName": "项津旭",
      "photoUrl": "",
      "userId": "05816301843889399017"
     },
     "user_tz": 300
    },
    "id": "HcpaDnuRukh6"
   },
   "outputs": [],
   "source": [
    "def delete_image(level_img = 5, category = 'train'):\n",
    "  image_path = my_path+'/Crop_Data/level'+str(level_img)+'/'+category+'/image/'\n",
    "  mask_path = my_path+'/Crop_Data/level'+str(level_img)+'/'+category+'/mask/'\n",
    "  image_files = os.listdir(image_path)\n",
    "  mask_files = os.listdir(mask_path)\n",
    "\n",
    "  print('Start deleting ' + category + ' images and masks, number of images:'+str(len(image_files))+' ,number of masks:'+str(len(mask_files )))\n",
    "\n",
    "  for file in image_files:\n",
    "    os.remove(image_path + file)\n",
    "  for file in os.listdir(mask_path):\n",
    "    os.remove(mask_path + file)\n",
    "  \n",
    "  print('Finish deleting ' + category + ' images and masks, number of images:'+str(len(os.listdir(image_path)))+' ,number of masks:'+str(len(os.listdir(mask_path))))\n",
    "  try:\n",
    "    os.remove(my_path+'/Crop_Data/level'+str(level)+'/'+category+'/image_start_pos.npy')\n",
    "    os.remove(my_path+'/Crop_Data/level'+str(level)+'/'+category+'/weights.npy')\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "def delete_image_all(level_img = 5):\n",
    "  delete_image(level_img, 'train')\n",
    "  delete_image(level_img, 'valid')\n",
    "  delete_image(level_img, 'test')\n",
    "  print('Finish deleting level', level_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5745,
     "status": "ok",
     "timestamp": 1604956133698,
     "user": {
      "displayName": "项津旭",
      "photoUrl": "",
      "userId": "05816301843889399017"
     },
     "user_tz": 300
    },
    "id": "361My36xuvFO",
    "outputId": "d9dd9ef4-6053-4635-fba0-2033b00886a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start deleting train images and masks, number of images:0 ,number of masks:0\n",
      "Finish deleting train images and masks, number of images:0 ,number of masks:0\n",
      "Start deleting valid images and masks, number of images:0 ,number of masks:0\n",
      "Finish deleting valid images and masks, number of images:0 ,number of masks:0\n",
      "Start deleting test images and masks, number of images:0 ,number of masks:0\n",
      "Finish deleting test images and masks, number of images:0 ,number of masks:0\n",
      "Finish deleting level 0\n",
      "Start deleting train images and masks, number of images:0 ,number of masks:0\n",
      "Finish deleting train images and masks, number of images:0 ,number of masks:0\n",
      "Start deleting valid images and masks, number of images:0 ,number of masks:0\n",
      "Finish deleting valid images and masks, number of images:0 ,number of masks:0\n",
      "Start deleting test images and masks, number of images:0 ,number of masks:0\n",
      "Finish deleting test images and masks, number of images:0 ,number of masks:0\n",
      "Finish deleting level 1\n",
      "Start deleting train images and masks, number of images:65291 ,number of masks:65291\n",
      "Finish deleting train images and masks, number of images:0 ,number of masks:0\n",
      "Start deleting valid images and masks, number of images:13335 ,number of masks:13335\n",
      "Finish deleting valid images and masks, number of images:0 ,number of masks:0\n",
      "Start deleting test images and masks, number of images:13697 ,number of masks:13697\n",
      "Finish deleting test images and masks, number of images:0 ,number of masks:0\n",
      "Finish deleting level 2\n",
      "Start deleting train images and masks, number of images:0 ,number of masks:0\n",
      "Finish deleting train images and masks, number of images:0 ,number of masks:0\n",
      "Start deleting valid images and masks, number of images:0 ,number of masks:0\n",
      "Finish deleting valid images and masks, number of images:0 ,number of masks:0\n",
      "Start deleting test images and masks, number of images:0 ,number of masks:0\n",
      "Finish deleting test images and masks, number of images:0 ,number of masks:0\n",
      "Finish deleting level 3\n",
      "Start deleting train images and masks, number of images:65291 ,number of masks:65291\n",
      "Finish deleting train images and masks, number of images:0 ,number of masks:0\n",
      "Start deleting valid images and masks, number of images:13335 ,number of masks:13335\n",
      "Finish deleting valid images and masks, number of images:0 ,number of masks:0\n",
      "Start deleting test images and masks, number of images:13697 ,number of masks:13697\n",
      "Finish deleting test images and masks, number of images:0 ,number of masks:0\n",
      "Finish deleting level 4\n",
      "Start deleting train images and masks, number of images:0 ,number of masks:0\n",
      "Finish deleting train images and masks, number of images:0 ,number of masks:0\n",
      "Start deleting valid images and masks, number of images:0 ,number of masks:0\n",
      "Finish deleting valid images and masks, number of images:0 ,number of masks:0\n",
      "Start deleting test images and masks, number of images:0 ,number of masks:0\n",
      "Finish deleting test images and masks, number of images:0 ,number of masks:0\n",
      "Finish deleting level 5\n",
      "Start deleting train images and masks, number of images:65291 ,number of masks:65291\n",
      "Finish deleting train images and masks, number of images:0 ,number of masks:0\n",
      "Start deleting valid images and masks, number of images:13335 ,number of masks:13335\n",
      "Finish deleting valid images and masks, number of images:0 ,number of masks:0\n",
      "Start deleting test images and masks, number of images:13697 ,number of masks:13697\n",
      "Finish deleting test images and masks, number of images:0 ,number of masks:0\n",
      "Finish deleting level 6\n",
      "Start deleting train images and masks, number of images:0 ,number of masks:0\n",
      "Finish deleting train images and masks, number of images:0 ,number of masks:0\n",
      "Start deleting valid images and masks, number of images:0 ,number of masks:0\n",
      "Finish deleting valid images and masks, number of images:0 ,number of masks:0\n",
      "Start deleting test images and masks, number of images:0 ,number of masks:0\n",
      "Finish deleting test images and masks, number of images:0 ,number of masks:0\n",
      "Finish deleting level 7\n"
     ]
    }
   ],
   "source": [
    "level = 2\n",
    "width_crop = 256\n",
    "height_crop = 256\n",
    "width_move = 192\n",
    "height_move = 192\n",
    "for i in range(8):\n",
    "  try:\n",
    "    delete_image_all(i)\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2667297,
     "status": "ok",
     "timestamp": 1604958809393,
     "user": {
      "displayName": "项津旭",
      "photoUrl": "",
      "userId": "05816301843889399017"
     },
     "user_tz": 300
    },
    "id": "35pdXFFRfS9n",
    "outputId": "9e85f6d7-5e20-4e55-f395-60973c385c6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read WSI from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_001.tif with width: 97792, height: 221184\n",
      "Read tumor mask from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_001_mask.tif\n",
      "Slide includes 10 levels Mask includes 9 levels\n",
      "Level 0, dimensions: (97792, 221184) downsample factor 1\n",
      "Level 1, dimensions: (48896, 110592) downsample factor 2\n",
      "Level 2, dimensions: (24448, 55296) downsample factor 4\n",
      "Level 3, dimensions: (12224, 27648) downsample factor 8\n",
      "Level 4, dimensions: (6112, 13824) downsample factor 16\n",
      "Level 5, dimensions: (3056, 6912) downsample factor 32\n",
      "Level 6, dimensions: (1528, 3456) downsample factor 64\n",
      "Level 7, dimensions: (764, 1728) downsample factor 128\n",
      "Level 8, dimensions: (382, 864) downsample factor 256\n",
      "Crop to 35340 images.\n",
      "....................\n",
      "Weight proportion： 177589965:1192243\n",
      "Generate 2728 valid images\n",
      "Finish tumor image: 001\n",
      "\n",
      "Read WSI from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_002.tif with width: 97792, height: 219648\n",
      "Read tumor mask from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_002_mask.tif\n",
      "Slide includes 10 levels Mask includes 9 levels\n",
      "Level 0, dimensions: (97792, 219648) downsample factor 1\n",
      "Level 1, dimensions: (48896, 109824) downsample factor 2\n",
      "Level 2, dimensions: (24448, 54912) downsample factor 4\n",
      "Level 3, dimensions: (12224, 27456) downsample factor 8\n",
      "Level 4, dimensions: (6112, 13728) downsample factor 16\n",
      "Level 5, dimensions: (3056, 6864) downsample factor 32\n",
      "Level 6, dimensions: (1528, 3432) downsample factor 64\n",
      "Level 7, dimensions: (764, 1716) downsample factor 128\n",
      "Level 8, dimensions: (382, 858) downsample factor 256\n",
      "Crop to 35092 images.\n",
      ".....................\n",
      "Weight proportion： 88831659:100693\n",
      "Generate 1357 valid images\n",
      "Finish tumor image: 002\n",
      "\n",
      "Read WSI from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_005.tif with width: 97792, height: 219648\n",
      "Read tumor mask from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_005_mask.tif\n",
      "Slide includes 10 levels Mask includes 9 levels\n",
      "Level 0, dimensions: (97792, 219648) downsample factor 1\n",
      "Level 1, dimensions: (48896, 109824) downsample factor 2\n",
      "Level 2, dimensions: (24448, 54912) downsample factor 4\n",
      "Level 3, dimensions: (12224, 27456) downsample factor 8\n",
      "Level 4, dimensions: (6112, 13728) downsample factor 16\n",
      "Level 5, dimensions: (3056, 6864) downsample factor 32\n",
      "Level 6, dimensions: (1528, 3432) downsample factor 64\n",
      "Level 7, dimensions: (764, 1716) downsample factor 128\n",
      "Level 8, dimensions: (382, 858) downsample factor 256\n",
      "Crop to 35092 images.\n",
      ".....................\n",
      "Weight proportion： 84789751:275977\n",
      "Generate 1298 valid images\n",
      "Finish tumor image: 005\n",
      "\n",
      "Read WSI from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_012.tif with width: 97792, height: 215552\n",
      "Read tumor mask from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_012_mask.tif\n",
      "Slide includes 10 levels Mask includes 9 levels\n",
      "Level 0, dimensions: (97792, 215552) downsample factor 1\n",
      "Level 1, dimensions: (48896, 107776) downsample factor 2\n",
      "Level 2, dimensions: (24448, 53888) downsample factor 4\n",
      "Level 3, dimensions: (12224, 26944) downsample factor 8\n",
      "Level 4, dimensions: (6112, 13472) downsample factor 16\n",
      "Level 5, dimensions: (3056, 6736) downsample factor 32\n",
      "Level 6, dimensions: (1528, 3368) downsample factor 64\n",
      "Level 7, dimensions: (764, 1684) downsample factor 128\n",
      "Level 8, dimensions: (382, 842) downsample factor 256\n",
      "Crop to 34348 images.\n",
      ".....................\n",
      "Weight proportion： 102976460:111668\n",
      "Generate 1573 valid images\n",
      "Finish tumor image: 012\n",
      "\n",
      "Read WSI from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_016.tif with width: 97792, height: 221184\n",
      "Read tumor mask from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_016_mask.tif\n",
      "Slide includes 10 levels Mask includes 9 levels\n",
      "Level 0, dimensions: (97792, 221184) downsample factor 1\n",
      "Level 1, dimensions: (48896, 110592) downsample factor 2\n",
      "Level 2, dimensions: (24448, 55296) downsample factor 4\n",
      "Level 3, dimensions: (12224, 27648) downsample factor 8\n",
      "Level 4, dimensions: (6112, 13824) downsample factor 16\n",
      "Level 5, dimensions: (3056, 6912) downsample factor 32\n",
      "Level 6, dimensions: (1528, 3456) downsample factor 64\n",
      "Level 7, dimensions: (764, 1728) downsample factor 128\n",
      "Level 8, dimensions: (382, 864) downsample factor 256\n",
      "Crop to 35340 images.\n",
      "....................\n",
      "Weight proportion： 85464458:17820278\n",
      "Generate 1576 valid images\n",
      "Finish tumor image: 016\n",
      "\n",
      "Read WSI from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_019.tif with width: 97792, height: 219648\n",
      "Read tumor mask from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_019_mask.tif\n",
      "Slide includes 10 levels Mask includes 9 levels\n",
      "Level 0, dimensions: (97792, 219648) downsample factor 1\n",
      "Level 1, dimensions: (48896, 109824) downsample factor 2\n",
      "Level 2, dimensions: (24448, 54912) downsample factor 4\n",
      "Level 3, dimensions: (12224, 27456) downsample factor 8\n",
      "Level 4, dimensions: (6112, 13728) downsample factor 16\n",
      "Level 5, dimensions: (3056, 6864) downsample factor 32\n",
      "Level 6, dimensions: (1528, 3432) downsample factor 64\n",
      "Level 7, dimensions: (764, 1716) downsample factor 128\n",
      "Level 8, dimensions: (382, 858) downsample factor 256\n",
      "Crop to 35092 images.\n",
      ".....................\n",
      "Weight proportion： 67313467:385221\n",
      "Generate 1033 valid images\n",
      "Finish tumor image: 019\n",
      "\n",
      "Read WSI from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_023.tif with width: 97792, height: 221184\n",
      "Read tumor mask from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_023_mask.tif\n",
      "Slide includes 10 levels Mask includes 9 levels\n",
      "Level 0, dimensions: (97792, 221184) downsample factor 1\n",
      "Level 1, dimensions: (48896, 110592) downsample factor 2\n",
      "Level 2, dimensions: (24448, 55296) downsample factor 4\n",
      "Level 3, dimensions: (12224, 27648) downsample factor 8\n",
      "Level 4, dimensions: (6112, 13824) downsample factor 16\n",
      "Level 5, dimensions: (3056, 6912) downsample factor 32\n",
      "Level 6, dimensions: (1528, 3456) downsample factor 64\n",
      "Level 7, dimensions: (764, 1728) downsample factor 128\n",
      "Level 8, dimensions: (382, 864) downsample factor 256\n",
      "Crop to 35340 images.\n",
      "....................\n",
      "Weight proportion： 95615036:198596\n",
      "Generate 1462 valid images\n",
      "Finish tumor image: 023\n",
      "\n",
      "Read WSI from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_031.tif with width: 97792, height: 221184\n",
      "Read tumor mask from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_031_mask.tif\n",
      "Slide includes 10 levels Mask includes 9 levels\n",
      "Level 0, dimensions: (97792, 221184) downsample factor 1\n",
      "Level 1, dimensions: (48896, 110592) downsample factor 2\n",
      "Level 2, dimensions: (24448, 55296) downsample factor 4\n",
      "Level 3, dimensions: (12224, 27648) downsample factor 8\n",
      "Level 4, dimensions: (6112, 13824) downsample factor 16\n",
      "Level 5, dimensions: (3056, 6912) downsample factor 32\n",
      "Level 6, dimensions: (1528, 3456) downsample factor 64\n",
      "Level 7, dimensions: (764, 1728) downsample factor 128\n",
      "Level 8, dimensions: (382, 864) downsample factor 256\n",
      "Crop to 35340 images.\n",
      "....................\n",
      "Weight proportion： 61953743:13871409\n",
      "Generate 1157 valid images\n",
      "Finish tumor image: 031\n",
      "\n",
      "Read WSI from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_035.tif with width: 97792, height: 221184\n",
      "Read tumor mask from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_035_mask.tif\n",
      "Slide includes 10 levels Mask includes 9 levels\n",
      "Level 0, dimensions: (97792, 221184) downsample factor 1\n",
      "Level 1, dimensions: (48896, 110592) downsample factor 2\n",
      "Level 2, dimensions: (24448, 55296) downsample factor 4\n",
      "Level 3, dimensions: (12224, 27648) downsample factor 8\n",
      "Level 4, dimensions: (6112, 13824) downsample factor 16\n",
      "Level 5, dimensions: (3056, 6912) downsample factor 32\n",
      "Level 6, dimensions: (1528, 3456) downsample factor 64\n",
      "Level 7, dimensions: (764, 1728) downsample factor 128\n",
      "Level 8, dimensions: (382, 864) downsample factor 256\n",
      "Crop to 35340 images.\n",
      "....................\n",
      "Weight proportion： 90288847:19761\n",
      "Generate 1378 valid images\n",
      "Finish tumor image: 035\n",
      "\n",
      "Read WSI from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_057.tif with width: 97792, height: 220672\n",
      "Read tumor mask from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_057_mask.tif\n",
      "Slide includes 10 levels Mask includes 9 levels\n",
      "Level 0, dimensions: (97792, 220672) downsample factor 1\n",
      "Level 1, dimensions: (48896, 110336) downsample factor 2\n",
      "Level 2, dimensions: (24448, 55168) downsample factor 4\n",
      "Level 3, dimensions: (12224, 27584) downsample factor 8\n",
      "Level 4, dimensions: (6112, 13792) downsample factor 16\n",
      "Level 5, dimensions: (3056, 6896) downsample factor 32\n",
      "Level 6, dimensions: (1528, 3448) downsample factor 64\n",
      "Level 7, dimensions: (764, 1724) downsample factor 128\n",
      "Level 8, dimensions: (382, 862) downsample factor 256\n",
      "Crop to 35216 images.\n",
      ".....................\n",
      "Weight proportion： 89981473:130527\n",
      "Generate 1375 valid images\n",
      "Finish tumor image: 057\n",
      "\n",
      "Read WSI from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_059.tif with width: 97280, height: 221184\n",
      "Read tumor mask from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_059_mask.tif\n",
      "Slide includes 10 levels Mask includes 9 levels\n",
      "Level 0, dimensions: (97280, 221184) downsample factor 1\n",
      "Level 1, dimensions: (48640, 110592) downsample factor 2\n",
      "Level 2, dimensions: (24320, 55296) downsample factor 4\n",
      "Level 3, dimensions: (12160, 27648) downsample factor 8\n",
      "Level 4, dimensions: (6080, 13824) downsample factor 16\n",
      "Level 5, dimensions: (3040, 6912) downsample factor 32\n",
      "Level 6, dimensions: (1520, 3456) downsample factor 64\n",
      "Level 7, dimensions: (760, 1728) downsample factor 128\n",
      "Level 8, dimensions: (380, 864) downsample factor 256\n",
      "Crop to 35055 images.\n",
      ".....................\n",
      "Weight proportion： 78480925:31203\n",
      "Generate 1198 valid images\n",
      "Finish tumor image: 059\n",
      "\n",
      "Read WSI from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_064.tif with width: 97792, height: 220672\n",
      "Read tumor mask from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_064_mask.tif\n",
      "Slide includes 10 levels Mask includes 9 levels\n",
      "Level 0, dimensions: (97792, 220672) downsample factor 1\n",
      "Level 1, dimensions: (48896, 110336) downsample factor 2\n",
      "Level 2, dimensions: (24448, 55168) downsample factor 4\n",
      "Level 3, dimensions: (12224, 27584) downsample factor 8\n",
      "Level 4, dimensions: (6112, 13792) downsample factor 16\n",
      "Level 5, dimensions: (3056, 6896) downsample factor 32\n",
      "Level 6, dimensions: (1528, 3448) downsample factor 64\n",
      "Level 7, dimensions: (764, 1724) downsample factor 128\n",
      "Level 8, dimensions: (382, 862) downsample factor 256\n",
      "Crop to 35216 images.\n",
      ".....................\n",
      "Weight proportion： 94808718:14833010\n",
      "Generate 1673 valid images\n",
      "Finish tumor image: 064\n",
      "\n",
      "Read WSI from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_075.tif with width: 90112, height: 78848\n",
      "Read tumor mask from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_075_mask.tif\n",
      "Slide includes 9 levels Mask includes 9 levels\n",
      "Level 0, dimensions: (90112, 78848) downsample factor 1\n",
      "Level 1, dimensions: (45056, 39424) downsample factor 2\n",
      "Level 2, dimensions: (22528, 19712) downsample factor 4\n",
      "Level 3, dimensions: (11264, 9856) downsample factor 8\n",
      "Level 4, dimensions: (5632, 4928) downsample factor 16\n",
      "Level 5, dimensions: (2816, 2464) downsample factor 32\n",
      "Level 6, dimensions: (1408, 1232) downsample factor 64\n",
      "Level 7, dimensions: (704, 616) downsample factor 128\n",
      "Level 8, dimensions: (352, 308) downsample factor 256\n",
      "Crop to 11286 images.\n",
      ".....................\n",
      "Weight proportion： 97132281:5366023\n",
      "Generate 1564 valid images\n",
      "Finish tumor image: 075\n",
      "\n",
      "Read WSI from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_078.tif with width: 94208, height: 111104\n",
      "Read tumor mask from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_078_mask.tif\n",
      "Slide includes 9 levels Mask includes 9 levels\n",
      "Level 0, dimensions: (94208, 111104) downsample factor 1\n",
      "Level 1, dimensions: (47104, 55552) downsample factor 2\n",
      "Level 2, dimensions: (23552, 27776) downsample factor 4\n",
      "Level 3, dimensions: (11776, 13888) downsample factor 8\n",
      "Level 4, dimensions: (5888, 6944) downsample factor 16\n",
      "Level 5, dimensions: (2944, 3472) downsample factor 32\n",
      "Level 6, dimensions: (1472, 1736) downsample factor 64\n",
      "Level 7, dimensions: (736, 868) downsample factor 128\n",
      "Level 8, dimensions: (368, 434) downsample factor 256\n",
      "Crop to 16779 images.\n",
      ".....................\n",
      "Weight proportion： 218240029:107801571\n",
      "Generate 4975 valid images\n",
      "Finish tumor image: 078\n",
      "\n",
      "Read WSI from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_081.tif with width: 90112, height: 100352\n",
      "Read tumor mask from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_081_mask.tif\n",
      "Slide includes 9 levels Mask includes 9 levels\n",
      "Level 0, dimensions: (90112, 100352) downsample factor 1\n",
      "Level 1, dimensions: (45056, 50176) downsample factor 2\n",
      "Level 2, dimensions: (22528, 25088) downsample factor 4\n",
      "Level 3, dimensions: (11264, 12544) downsample factor 8\n",
      "Level 4, dimensions: (5632, 6272) downsample factor 16\n",
      "Level 5, dimensions: (2816, 3136) downsample factor 32\n",
      "Level 6, dimensions: (1408, 1568) downsample factor 64\n",
      "Level 7, dimensions: (704, 784) downsample factor 128\n",
      "Level 8, dimensions: (352, 392) downsample factor 256\n",
      "Crop to 14478 images.\n",
      ".....................\n",
      "Weight proportion： 148968651:60213\n",
      "Generate 2274 valid images\n",
      "Finish tumor image: 081\n",
      "\n",
      "Weight proportion on train data： 1582435463:162198393\n",
      "A total of 26621 train data is generated.\n",
      "\n",
      "Read WSI from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_084.tif with width: 65536, height: 86016\n",
      "Read tumor mask from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_084_mask.tif\n",
      "Slide includes 9 levels Mask includes 8 levels\n",
      "Level 0, dimensions: (65536, 86016) downsample factor 1\n",
      "Level 1, dimensions: (32768, 43008) downsample factor 2\n",
      "Level 2, dimensions: (16384, 21504) downsample factor 4\n",
      "Level 3, dimensions: (8192, 10752) downsample factor 8\n",
      "Level 4, dimensions: (4096, 5376) downsample factor 16\n",
      "Level 5, dimensions: (2048, 2688) downsample factor 32\n",
      "Level 6, dimensions: (1024, 1344) downsample factor 64\n",
      "Level 7, dimensions: (512, 672) downsample factor 128\n",
      "Crop to 8938 images.\n",
      ".....................\n",
      "Weight proportion： 114911977:3577111\n",
      "Generate 1808 valid images\n",
      "Finish tumor image: 084\n",
      "\n",
      "Read WSI from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_091.tif with width: 61440, height: 53760\n",
      "Read tumor mask from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_091_mask.tif\n",
      "Slide includes 8 levels Mask includes 8 levels\n",
      "Level 0, dimensions: (61440, 53760) downsample factor 1\n",
      "Level 1, dimensions: (30720, 26880) downsample factor 2\n",
      "Level 2, dimensions: (15360, 13440) downsample factor 4\n",
      "Level 3, dimensions: (7680, 6720) downsample factor 8\n",
      "Level 4, dimensions: (3840, 3360) downsample factor 16\n",
      "Level 5, dimensions: (1920, 1680) downsample factor 32\n",
      "Level 6, dimensions: (960, 840) downsample factor 64\n",
      "Level 7, dimensions: (480, 420) downsample factor 128\n",
      "Crop to 5159 images.\n",
      ".....................\n",
      "Weight proportion： 73764955:5336997\n",
      "Generate 1207 valid images\n",
      "Finish tumor image: 091\n",
      "\n",
      "Read WSI from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_094.tif with width: 118784, height: 100352\n",
      "Read tumor mask from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_094_mask.tif\n",
      "Slide includes 9 levels Mask includes 9 levels\n",
      "Level 0, dimensions: (118784, 100352) downsample factor 1\n",
      "Level 1, dimensions: (59392, 50176) downsample factor 2\n",
      "Level 2, dimensions: (29696, 25088) downsample factor 4\n",
      "Level 3, dimensions: (14848, 12544) downsample factor 8\n",
      "Level 4, dimensions: (7424, 6272) downsample factor 16\n",
      "Level 5, dimensions: (3712, 3136) downsample factor 32\n",
      "Level 6, dimensions: (1856, 1568) downsample factor 64\n",
      "Level 7, dimensions: (928, 784) downsample factor 128\n",
      "Level 8, dimensions: (464, 392) downsample factor 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crop to 19177 images.\n",
      ".....................\n",
      "Weight proportion： 237277953:6909183\n",
      "Generate 3726 valid images\n",
      "Finish tumor image: 094\n",
      "\n",
      "Weight proportion on valid data： 425954885:15823291\n",
      "A total of 6741 valid data is generated.\n",
      "\n",
      "Read WSI from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_096.tif with width: 131072, height: 71680\n",
      "Read tumor mask from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_096_mask.tif\n",
      "Slide includes 9 levels Mask includes 9 levels\n",
      "Level 0, dimensions: (131072, 71680) downsample factor 1\n",
      "Level 1, dimensions: (65536, 35840) downsample factor 2\n",
      "Level 2, dimensions: (32768, 17920) downsample factor 4\n",
      "Level 3, dimensions: (16384, 8960) downsample factor 8\n",
      "Level 4, dimensions: (8192, 4480) downsample factor 16\n",
      "Level 5, dimensions: (4096, 2240) downsample factor 32\n",
      "Level 6, dimensions: (2048, 1120) downsample factor 64\n",
      "Level 7, dimensions: (1024, 560) downsample factor 128\n",
      "Level 8, dimensions: (512, 280) downsample factor 256\n",
      "Crop to 15030 images.\n",
      ".....................\n",
      "Weight proportion： 174640367:2503441\n",
      "Generate 2703 valid images\n",
      "Finish tumor image: 096\n",
      "\n",
      "Read WSI from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_101.tif with width: 139264, height: 71680\n",
      "Read tumor mask from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_101_mask.tif\n",
      "Slide includes 10 levels Mask includes 9 levels\n",
      "Level 0, dimensions: (139264, 71680) downsample factor 1\n",
      "Level 1, dimensions: (69632, 35840) downsample factor 2\n",
      "Level 2, dimensions: (34816, 17920) downsample factor 4\n",
      "Level 3, dimensions: (17408, 8960) downsample factor 8\n",
      "Level 4, dimensions: (8704, 4480) downsample factor 16\n",
      "Level 5, dimensions: (4352, 2240) downsample factor 32\n",
      "Level 6, dimensions: (2176, 1120) downsample factor 64\n",
      "Level 7, dimensions: (1088, 560) downsample factor 128\n",
      "Level 8, dimensions: (544, 280) downsample factor 256\n",
      "Crop to 16020 images.\n",
      "....................\n",
      "Weight proportion： 160155038:21379682\n",
      "Generate 2770 valid images\n",
      "Finish tumor image: 101\n",
      "\n",
      "Read WSI from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_110.tif with width: 94208, height: 71680\n",
      "Read tumor mask from C:/course/4995 Applied Deeping Learning/homework/Project/Project Data/tumor_110_mask.tif\n",
      "Slide includes 9 levels Mask includes 9 levels\n",
      "Level 0, dimensions: (94208, 71680) downsample factor 1\n",
      "Level 1, dimensions: (47104, 35840) downsample factor 2\n",
      "Level 2, dimensions: (23552, 17920) downsample factor 4\n",
      "Level 3, dimensions: (11776, 8960) downsample factor 8\n",
      "Level 4, dimensions: (5888, 4480) downsample factor 16\n",
      "Level 5, dimensions: (2944, 2240) downsample factor 32\n",
      "Level 6, dimensions: (1472, 1120) downsample factor 64\n",
      "Level 7, dimensions: (736, 560) downsample factor 128\n",
      "Level 8, dimensions: (368, 280) downsample factor 256\n",
      "Crop to 10710 images.\n",
      ".....................\n",
      "Weight proportion： 113030375:117000985\n",
      "Generate 3510 valid images\n",
      "Finish tumor image: 110\n",
      "\n",
      "Weight proportion on test data： 447825780:140884108\n",
      "A total of 8983 test data is generated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tumor_num_train = [1,2,5,12,16,19,23,31,35,57,59,64,75,78,81]\n",
    "tumor_num_train = [str(i).zfill(3) for i in tumor_num_train]\n",
    "tumor_num_valid = [84,91,94]\n",
    "tumor_num_valid = [str(i).zfill(3) for i in tumor_num_valid]\n",
    "tumor_num_test = [96,101,110]\n",
    "tumor_num_test = [str(i).zfill(3) for i in tumor_num_test]\n",
    "train_start_pos = crop_train_test_data(tumor_num_train, level_img=level, width_crop=width_crop, height_crop=height_crop, width_move=width_move, height_move=height_move, category ='train')\n",
    "valid_start_pos = crop_train_test_data(tumor_num_valid, level_img=level, width_crop=width_crop, height_crop=height_crop, width_move=width_move, height_move=height_move, category ='valid')\n",
    "test_start_pos = crop_train_test_data(tumor_num_test, level_img=level, width_crop=width_crop, height_crop=height_crop, width_move=width_move, height_move=height_move, category ='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "get_image.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
